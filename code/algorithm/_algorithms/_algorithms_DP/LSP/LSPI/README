%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Copyright 2000-2002 
%
% Michail G. Lagoudakis (mgl@cs.duke.edu)
% Ronald Parr (parr@cs.duke.edu)
%
% Department of Computer Science
% Box 90129
% Duke University
% Durham, NC 27708
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  DISCLAIMER

    This Program is  provided by Duke University and  the authors as a
    service to the research community.  It is provided without cost or
    restrictions,  except  for  the  User's acknowledgement  that  the
    Program is provided on an  "As Is" basis and User understands that
    Duke  University  and  the  authors  make no  express  or  implied
    warranty   of  any   kind.   Duke   University  and   the  authors
    specifically disclaim  any implied warranty  or merchantability or
    fitness for  a particular purpose, and make  no representations or
    warranties  that the  Program will  not infringe  the intellectual
    property rights of  others. The User agrees to  indemnify and hold
    harmless Duke University and the  authors from and against any and
    all liability arising out of User's use of the Program.



README file for LSPI : Least-Squares Policy Iteration
 

Thanks for using our LSPI software!
Check back at http://www.cs.duke.edu/~mgl/LSPI/ for updates
 

Installation
---------------

1. Download the file lspi.tar.gz
2. Decompress: gunzip lspi.tar.gz
3. Unpack: tar -xvf lspi.tar


Requirements
---------------

1. MATLAB V.6 or higher 
   (other versions may work fine, but have not been tested). 
2. Sufficient main and secondary memory 
   (depending on the needs of your target domain)

Recommended:

1. MATLAB mex compiler (compiles C code to MATLAB mex files)
   for speeding up parts of the code (not so much the LSPI functions
   as much as the domain functions, especially the function that
   computes the basis).


LSPI functions
----------------

The main functions in the LSPI distribution are the following: 

collect_samples.m : A function that collects samples from a domain

lspi.m : The main LSPI function that learns a good policy for a
         domain, given a set of samples

These are the two top-level functions. The others are rarely called
directly. However, all functions have help files. Just type

  >> help <function>

to get more information.


Basic concepts and Data structures
-------------------

LSPI can be used to learning good decision making policies in problems
where the underlying process can be modeled as a Markov Decision
Process (MDP). The main entities of the problem and their
representation within this code are described below:


State

An arbitrary description of the state of the process. It can be a
number, an array of numbers, a structure, a string, etc. It doesn't
really matter for LSPI how the state is represented, since LSPI "sees"
the state only through the basis functions.


Action

An integer in the range [1,|A|], where A is the set of all possible
actions. Composite actions, like a=[1 4 5] are not supported by this
version of LSPI. However, it is straightforward to index all the
actions in your domain by a single integer.


Sample (state,action,reward,nextstate)

This is a single piece of experience. It is represented as a structure
with the following fields:

state     : Arbitrary description of state
action    : An integer in [1,|A|]
reward    : A real value 
nextstate : Arbitrary description of nextstate
absorb    : Absorbing nextstate? (boolean 0 or 1)


Q value function

No Q value function is explicitly represented within LSPI. Only
approximate Q value functions are considered of the following form
(linear architectures):

Q(state,action) = sum_{i=1}^k phi_i(state, action) * w_i

Only the weights w_i are stored within LSPI and Q values are computed
if needed by the linear combination above.


Basis

This is an arbitrary set of k functions defined over pairs (state,action).
phi_i(state,action) is the i-th function evaluated at (state,action).
phi(state,action) is a column vector with all k functions evaluated at
(state,action).


Policy

Policies are mappings from states to actions. Policies are not
explicitly represented within LSPI, but only implicitly through a set
of weights:

pi(state) = max_{a in A} Q(state,a) = max_{a in A} phi(state,a)*w

Such a policy is represented as a structure with the following fields: 

explore  : Exploration rate (real number)
discount : Discount factor (real number)
actions  : Total numbers of actions, |A|
basis    : The function handle for the basis
           associated with this policy
weights  : A column array of weights 
           (one for each basis function)



Creating a domain
-------------------

To use LSPI you first need to create a domain. There are certain
guidelines for doing so, designed to make the interface with LSPI as
generic as possible.

1. Choose a name for the domain, e.g. 'chain', 'pendulum', 'bicycle', ... 

   This name will be provided to the LSPI functions, which, in turn,
   will look for certain functions. These are:

   domain_simulator.m
   domain_basis.m
   domain_initialize_policy.m
   domain_initialize_state.m
   domain_print_info.m

   For example, assuming that domain='chain', these functions are the
   following:

   chain_simulator.m
   chain_basis.m
   chain_initialize_policy.m
   chain_initialize_state.m
   chain_print_info.m

2. Provide these functions. Here is the API for each one of them: 
   
 * function [nextstate, reward, absorb] = domain_simulator(state, action)
 
   Input: 

     - state : An arbitrary description of state (number, array of
               numbers, struct, etc.) - Your choice
     - action: An integer in [1,|A|] indicating an action taken in "state"

   Output:

     Depending on the input arguments:

     [nextstate, reward, absorb] = domain_simulator()

       The function should initialize the simulator and return its
       current state as "nextstate". "reward" and "absorb" can be
       arbitrary, but better set them to 0.

     [nextstate, reward, absorb] = domain_simulator(state)

       The function should initialize the simulator to "state" and
       return its current state as "nextstate". "reward" and "absorb"
       can be arbitrary, but better set them to 0.

     [nextstate, reward, absorb] = domain_simulator(state,action)

	The function should simulate a transition from "state" under
        "action" and return the "nextstate", the "reward" received,
        and the "absorb" flag (1 if "nextstate" is an absorbing state,
        0 otherwise).


 * function phi = domain_basis(state, action)

     This function computes the basis functions for any given pair
     ("state","action"). Actually there is no naming restriction for
     this function, since its name is provided to LSPI as input, so
     that many such functions can be tried for any given domain. The
     name "domain_basis" is used to indicate that at least one such
     function must be present.

     The output of the function depends on the input arguments. The
     following calls are expected:

     phi = domain_basis

        When called without any arguments, "phi" must be an integer
        indicating the total number of basis functions.

     phi = domain_basis(state, action)

        With both arguments, "phi" must be a column vector of length
        equal to the total number of basis functions. Each entry is
        the corresponding basis function evaluated at the pair (state,
        action).


 * function policy = domain_initialize_policy(explore, discount, basis)

     Policies throughout LSPI are supposed to be structures with the
     following fields (at least):

        explore  : Exploration rate (real number)
        discount : Discount factor (real number)
        actions  : Total numbers of actions, |A|
        basis    : The function handle for the basis
                   associated with this policy
        weights  : A column array of weights 
                   (one for each basis function)

    The function domain_initialize_policy should generate a new
    "policy". "explore" (a real number), "discount" (a real number),
    and "basis" (a function handle, e.g. @domain_phi) are provided as
    inputs. "actions" is a feature of the domain and the "weights" can
    be initialized arbitrarily. However, policy.weights has to be a
    column vector of length equal to the total number of basis
    functions (that can be obtained by a call to "basis" without any
    arguments). The values in "policy.weights" are typically
    initialized to zeros, but anything else is also acceptable.


 * function initial_state = domain_initialize_state(simulator)

     This function returns an initial state for an episode that will
     be used for sample collection. This kind of initialization is
     distinguished from normal initialization of the simulator as for
     many domains it is necessary to add some noise to the initial
     state when collecting samples.

     The function takes a function handle to a simulator as an input
     and returns a state.

     If there is no need for such special initialization, just define
     this function as follows:

        initial_state  = feval(simulator);


 * function domain_print_info(all_policies)

     This function is called by LSPI at the end of each LSPI iteration
     and can be used to visualize properties of the policies so far
     (or save information, or test a policy, or whatever). You will
     find this call useful for long LSPI runs that take hours and you
     need some idea of where the iteration is going.

     The input argument is a cell array with all policies produced
     so far during a run of LSPI with the most recent at the end.

     It you don't need such a function, just define it as: 

        return


Running LSPI 
---------------

1. Start MATLAB in the directory DOMAIN where you have all your domain files. 

2. Add the path to the LSPI directory, e.g.

   >> addpath /home/mgl/LSPI/

   You have to give the full path. Relative paths, like ../LSPI are
   not allowed.

3. You are ready to start issuing LSPI commands for your domain. 



LSPI usage
--------------

For a quick test, assuming you are running the chain domain, try
issuing the following command:

  >> [pol, allpol] = chain_learn;

This a typical LSPI run and should run without any errors. 

Typically, when using LSPI, you first make a call to "collect_samples"
to collect some samples and then a call to "lspi" to learn a
policy. These two operations are separated so that you can fix the
sample set and then try several runs of LSPI with different basis
functions.

The function chain_learn.m automates the usage of these two
functions. You can write a similar function for each of your
domains. There are several arguments you can pass to chain_learn to
achieve what you want. A description is given in the README file of
the chain domain.

The usage of these two main functions (collect_samples and lspi) is
shown below:

-------------------------------------------------------------------

new_samples = collect_samples(domain, maxepisodes, maxsteps, policy)

Collects samples from the "domain" using the "policy" by running at
most "maxepisodes" episodes each of which is at most "maxsteps"
steps long.

Input:

domain - A string containing the name of the domain
         This string should be the prefix for all related
         functions, for example if domain is 'chain' functions
         should be chain_initialize_policy, chain_simulator,
         etc.

maxepisodes - An integer indicating the maximum number of
              episodes from which samples are collected.

maxsteps - An integer indicating the maximum number of steps of each
           episode (an episode may finish earlier if an absorbing
           state is encountered).

policy - (optional argument) A policy to be used for collecting the
         samples. It should be given as a struct with the following
         fields (at least):

         explore  : Exploration rate (real number)
         discount : Discount factor (real number)
         actions  : Total numbers of actions, |A|
         basis    : The function handle for the basis
                    associated with this policy
         weights  : A column array of weights 
                    (one for each basis function)

         If a policy is not provided, samples will be collected by a
         purely random policy initialized with "explore"=1.0,
         "discount" and "basis" some dummy values, and "actions" and
         "weights" as suggested by the domain (in the
         domain_initialize_policy function.

Output:

new_samples - An array of the collected samples. Each entry
              new_samples(i) has the following form:

              new_samples(i).state     : Arbitrary description of state
              new_samples(i).action    : An integer in [1,|A|]
              new_samples(i).reward    : A real value
              new_samples(i).nextstate : Arbitrary description
              new_samples(i).absorb    : Absorbing nextstate? (0 or 1)


--------------------------------------------------------------------

[policy, all_policies] = lspi(domain, algorithm, maxiterations,
                              epsilon, samples, basis, discount,
                              initial_policy)

LSPI : Least-Squares Policy Iteration

Finds a good policy given a set of samples and a basis

Input:

domain - A string containing the name of the domain
         This string should be the prefix for all related
         functions, for example if domain is 'chain' functions
         should be chain_initialize_policy, chain_simulator,
         etc.

algorithm - This is a number that indicates which evaluation
            algorithm should be used (see the paper):

            1-lsq       : The regular LSQ (incremental)
            2-lsqfast   : A fast version of LSQ (uses more space)
            3-lsqbe     : LSQ with Bellman error minimization 
            4-lsqbefast : A fast version of LSQBE (more space)

            LSQ is the evaluation algorithm for regular
            LSPI. Use lsqfast in general, unless you have
            really big sample sets. LSQBE is provided for
            comparison purposes and completeness.

maxiterations - An integer indicating the maximum number of
                LSPI iterations 

epsilon - A small real number used as the termination
          criterion. LSPI converges if the distance between
          weights of consecutive iterations is less than
          epsilon.

samples - The sample set. This should be an array where each
           entry samples(i) has the following form: 

           samples(i).state     : Arbitrary description of state
           samples(i).action    : An integer in [1,|A|]
           samples(i).reward    : A real value
           samples(i).nextstate : Arbitrary description
           samples(i).absorb    : Absorbing nextstate? (0 or 1)

basis - The function that computes the basis for a given pair
        (state, action) given as a function handle
        (e.g. @chain_phi) or as a string (e.g. 'chain_phi').

discount - A real number in (0,1] to be used as the discount factor

initial_policy - (optional argument) An initial policy for
                 LSPI. It should be given as a struct with the
                 following fields (at least):

                 explore  : Exploration rate (real number)
                 discount : Discount factor (real number)
                 actions  : Total numbers of actions, |A|
                 basis    : The function handle for the basis
                            associated with this policy
                 weights  : A column array of weights 
                            (one for each basis function)

                 If initial_policy is not provided it is initialized
                 to a policy with "explore"=0.0, "discount" and
                 "basis" as provided above, and "actions" and
                 "weights" as suggested by the domain (in the
                 domain_initialize_policy function.

Output:

policy - The learned policy (same struct as above)

all_policies - A cell array of size (iterations+1) containing
               all the intermediate policies at each LSPI
               iteration, including the initial policy. 

-----------------------------------------------------------------


We hope you'll enjoy working with LSPI! 
