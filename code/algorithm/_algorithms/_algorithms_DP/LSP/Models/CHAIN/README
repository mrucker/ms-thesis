%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Copyright 2000-2002 
%
% Michail G. Lagoudakis (mgl@cs.duke.edu)
% Ronald Parr (parr@cs.duke.edu)
%
% Department of Computer Science
% Box 90129
% Duke University
% Durham, NC 27708
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  DISCLAIMER

    This Program is  provided by Duke University and  the authors as a
    service to the research community.  It is provided without cost or
    restrictions,  except  for  the  User's acknowledgement  that  the
    Program is provided on an  "As Is" basis and User understands that
    Duke  University  and  the  authors  make no  express  or  implied
    warranty   of  any   kind.   Duke   University  and   the  authors
    specifically disclaim  any implied warranty  or merchantability or
    fitness for  a particular purpose, and make  no representations or
    warranties  that the  Program will  not infringe  the intellectual
    property rights of  others. The User agrees to  indemnify and hold
    harmless Duke University and the  authors from and against any and
    all liability arising out of User's use of the Program.



README file for the CHAIN domain
 

Thanks for using our LSPI software!
Check back at http://www.cs.duke.edu/~mgl/LSPI/ for updates
 

Installation
---------------

1. Download the file chain.tar.gz
2. Decompress: gunzip chain.tar.gz
3. Unpack: tar -xvf chain.tar


Requirements
---------------

1. MATLAB V.5 or higher
2. Sufficient main and secondary memory 
   (depending on your target domain)

Recommended:

1. MATLAB mex compiler (compiles C code to MATLAB mex files)
   for speeding up the code


CHAIN functions
----------------

All functions in the CHAIN distribution have help files. Just type

  >> help <function>

to get more information.


Running LSPI 
---------------

1. Start MATLAB in the directory CHAIN. 

2. Add the path to the LSPI directory, e.g.

   >> addpath /home/mgl/LSPI/

   You have to give the full path. Relative paths, like ../LSPI are
   not allowed.

3. You are ready to start issuing LSPI commands for your domain. 



Usage
--------------

For a quick test try issuing the following command:

  >> [pol, allpol] = chain_learn;

This a typical LSPI run and should run without any errors. 

The usage of the function chain_learn.m is shown below: 

-------------------------------------------------------------

[final_policy, all_policies, samples] = chain_learn(maxiterations, ...
						  epsilon, samples, ...
						  maxepisodes, ...
						  maxsteps, discount, ...
						  basis, algorithm, policy)

Runs LSPI for the chain domain

Input:

maxiterations - An integer indicating the maximum number of
                LSPI iterations (default = 8)

epsilon - A small real number used as the termination
          criterion. LSPI converges if the distance between
          weights of consequtive iterations is less than
          epsilon (default = 10^-5)

samples - The sample set. This should be an array where each
           entry samples(i) has the following form: 

           samples(i).state     : Arbitrary description of state
           samples(i).action    : An integer in [1,|A|]
           samples(i).reward    : A real value
           samples(i).nextstate : Arbitrary description
           samples(i).absorb    : Absorbing nextstate? (0 or 1)

           (default = [] - empty)

maxepisodes - An integer indicating the maximum number of
              episodes from which (additional) samples will be
              collected.
              (default = 10, if samples is empty, 0 otherwise)

maxsteps - An integer indicating the maximum number of steps of each
           episode (an episode may finish earlier if an absorbing
           state is encountered). 
           (default = 50)

discount - A real number in (0,1] to be used as the discount factor
           (default = 0.9)

basis - The function that computes the basis for a given pair
        (state, action) given as a function handle
        (e.g. @chain_phi) or as a string (e.g. 'chain_phi')
        (default = @chain_basis_pol)

algorithm - This is a number that indicates which evaluation
            algorithm should be used (see the paper):

            1-lsq       : The regular LSQ (incremental)
            2-lsqfast   : A fast version of LSQ (uses more space)
            3-lsqbe     : LSQ with Bellman error minimization 
            4-lsqbefast : A fast version of LSQBE (more space)

            LSQ is the evaluation algorithm for regular
            LSPI. Use lsqfast in general, unless you have
            really big sample sets. LSQBE is provided for
            comparison purposes and completeness.
            (default = 2)

policy - (optional argument) A policy to be used for collecting the
         (additional) samples and as the initial policy for LSPI. It
         should be given as a struct with the following fields (at
         least):

         explore  : Exploration rate (real number)
         discount : Discount factor (real number)
         actions  : Total numbers of actions, |A|
         basis    : The function handle for the basis
                    associated with this policy
         weights  : A column array of weights 
                    (one for each basis function)

         If a policy is not provided, samples will be collected by a
         purely random policy initialized with "explore"=1.0,
         "discount" and "basis" some dummy values, and "actions" and
         "weights" as suggested by the chain domain (in the
         chain_initialize_policy function. Notice that the
         "basis" used by this policy can be different from the
         "basis" above that is used for the LSPI iteration. 


Output:

final_policy - The learned policy (same struct as above)

all_policies - A cell array of size (iterations+1) containing
               all the intermediate policies at each LSPI
               iteration, including the initial policy. 

samples     - The set of all samples used for this run. Each entry
              samples(i) has the following form:

              samples(i).state     : Arbitrary description of state
              samples(i).action    : An integer in [1,|A|]
              samples(i).reward    : A real value
              samples(i).nextstate : Arbitrary description
              samples(i).absorb    : Absorbing nextstate? (0 or 1)

-------------------------------------------------------------

Here are some typical examples : 

1. I just want to run it with the default values: 

   >> [pol, allpol, sam] = chain_learn;

2. I only want to collect samples from 10 episodes, each 100 steps long

   >> [pol, allpol, sam] = chain_learn(0,0,[],10,100);

3. I have some samples in the array mysam and I want to run LSPI on
   this set for max 8 iterations and epsilon=10^-3. I also want to use
   a discount factor of 0.8 and the "chain_basis_rbf" basis.
 
   >> [pol, allpol, sam] = chain_learn(8,10^-3,mysam,0,0,0.8,@chain_basis_rbf);

4. In addition to the samples in mysam I want to collect some more
   samples (3 episodes, 200 steps max each) using a policy stored in mypol

   >> [pol, allpol, sam] = chain_learn(0,10^-3,mysam,3,200,0.8,@chain_basis_rbf, 2, mypol);

4. In addition to the samples in mysam I want to collect some more
   samples using a policy stored in mypol (3 episodes, 200 steps max
   each) and then run LSPI on this combined set, starting the
   iteration with the policy mypol, but using the chain_basis_pol
   basis (mypol uses the chain_basis_rbf):

   >> [pol, allpol, sam] = chain_learn(8,10^-3,mysam,3,200,0.8,'chain_basis_pol', 2, mypol);

5. I want to collect samples (1 episode, 3000 steps) using policy
   mypol and then run LSPI on that sample set using the basis
   chain_basis_pol_C.

   >> [pol, allpol, sam] = chain_learn(8,10^-3,[],1,3000,0.8,'chain_basis_pol_C', 2, mypol);

... and so on! You get the idea! 

-------------------------------------------------------

Hope you enjoy working with LSPI!!!
